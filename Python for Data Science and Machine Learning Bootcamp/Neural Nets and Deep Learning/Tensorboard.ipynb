{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1ea340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7d0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA/cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e95cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "5                 0.07613  ...          23.75           103.40       741.6   \n",
       "6                 0.05742  ...          27.66           153.20      1606.0   \n",
       "7                 0.07451  ...          28.14           110.60       897.0   \n",
       "8                 0.07389  ...          30.73           106.20       739.3   \n",
       "9                 0.08243  ...          40.68            97.65       711.4   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "5            0.1791             0.5249           0.5355                0.1741   \n",
       "6            0.1442             0.2576           0.3784                0.1932   \n",
       "7            0.1654             0.3682           0.2678                0.1556   \n",
       "8            0.1703             0.5401           0.5390                0.2060   \n",
       "9            0.1853             1.0580           1.1050                0.2210   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  benign_0__mal_1  \n",
       "0          0.4601                  0.11890                0  \n",
       "1          0.2750                  0.08902                0  \n",
       "2          0.3613                  0.08758                0  \n",
       "3          0.6638                  0.17300                0  \n",
       "4          0.2364                  0.07678                0  \n",
       "5          0.3985                  0.12440                0  \n",
       "6          0.3063                  0.08368                0  \n",
       "7          0.3196                  0.11510                0  \n",
       "8          0.4378                  0.10720                0  \n",
       "9          0.4366                  0.20750                0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6421b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c64e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9d68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4d8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca2835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8785c6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abc317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb2b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d948da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e70b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f635c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tylercoen/Desktop/Certifications/Coding Courses/Python/Python for Data Science and Machine Learning Bootcamp/Neural Nets and Deep Learning'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f543817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31378f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-03-07==1801'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%Y-%m-%d==%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "371de4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'logs/fit'\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d==%H%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e6b7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = log_directory + '/' + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9b31f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 18:05:18.563180: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-03-07 18:05:18.563194: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-03-07 18:05:18.563855: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,write_graph=True,write_images=True,\n",
    "                   update_freq='epoch',profile_batch=2,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "392dd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e92b7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      " 1/14 [=>............................] - ETA: 6s - loss: 27.8189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 18:08:22.047688: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-03-07 18:08:22.047699: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-03-07 18:08:22.050309: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-03-07 18:08:22.053526: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-03-07 18:08:22.056860: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/fit/2024-03-07==1803/plugins/profile/2024_03_07_18_08_22/Tylers-iMac.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 19ms/step - loss: 23.1610 - val_loss: 0.6867\n",
      "Epoch 2/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.7694 - val_loss: 0.6848\n",
      "Epoch 3/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.5148 - val_loss: 0.6823\n",
      "Epoch 4/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.3215 - val_loss: 0.6811\n",
      "Epoch 5/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.2648 - val_loss: 0.6800\n",
      "Epoch 6/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.7461 - val_loss: 0.6792\n",
      "Epoch 7/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0890 - val_loss: 0.6782\n",
      "Epoch 8/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6543 - val_loss: 0.6774\n",
      "Epoch 9/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.1764 - val_loss: 0.6768\n",
      "Epoch 10/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.9465 - val_loss: 0.6762\n",
      "Epoch 11/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3690 - val_loss: 0.6756\n",
      "Epoch 12/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.8274 - val_loss: 0.6751\n",
      "Epoch 13/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.2884 - val_loss: 0.6741\n",
      "Epoch 14/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.4126 - val_loss: 0.6734\n",
      "Epoch 15/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.3301 - val_loss: 0.6728\n",
      "Epoch 16/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1.4209 - val_loss: 0.6724\n",
      "Epoch 17/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.5614 - val_loss: 0.6720\n",
      "Epoch 18/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.7114 - val_loss: 0.6715\n",
      "Epoch 19/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.4118 - val_loss: 0.6706\n",
      "Epoch 20/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.5045 - val_loss: 0.6700\n",
      "Epoch 21/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.2914 - val_loss: 0.6695\n",
      "Epoch 22/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.0803 - val_loss: 0.6690\n",
      "Epoch 23/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.1759 - val_loss: 0.6687\n",
      "Epoch 24/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.1776 - val_loss: 0.6685\n",
      "Epoch 25/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.1531 - val_loss: 0.6681\n",
      "Epoch 26/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.3931 - val_loss: 0.6678\n",
      "Epoch 27/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.1338 - val_loss: 0.6676\n",
      "Epoch 28/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1.1473 - val_loss: 0.6672\n",
      "Epoch 29/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.8786 - val_loss: 0.6668\n",
      "Epoch 30/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8764 - val_loss: 0.6665\n",
      "Epoch 31/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.1937 - val_loss: 0.6661\n",
      "Epoch 32/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8950 - val_loss: 0.6661\n",
      "Epoch 33/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.0051 - val_loss: 0.6657\n",
      "Epoch 34/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7855 - val_loss: 0.6653\n",
      "Epoch 35/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.9043 - val_loss: 0.6653\n",
      "Epoch 36/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7133 - val_loss: 0.6651\n",
      "Epoch 37/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.9373 - val_loss: 0.6649\n",
      "Epoch 38/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8139 - val_loss: 0.6649\n",
      "Epoch 39/600\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7639 - val_loss: 0.6649\n",
      "Epoch 40/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6958 - val_loss: 0.6648\n",
      "Epoch 41/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7742 - val_loss: 0.6646\n",
      "Epoch 42/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6736 - val_loss: 0.6645\n",
      "Epoch 43/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6970 - val_loss: 0.6646\n",
      "Epoch 44/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6971 - val_loss: 0.6646\n",
      "Epoch 45/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7874 - val_loss: 0.6649\n",
      "Epoch 46/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.9029 - val_loss: 0.6650\n",
      "Epoch 47/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7188 - val_loss: 0.6652\n",
      "Epoch 48/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7817 - val_loss: 0.6653\n",
      "Epoch 49/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6899 - val_loss: 0.6655\n",
      "Epoch 50/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6508 - val_loss: 0.6657\n",
      "Epoch 51/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6708 - val_loss: 0.6656\n",
      "Epoch 52/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6933 - val_loss: 0.6656\n",
      "Epoch 53/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7222 - val_loss: 0.6661\n",
      "Epoch 54/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7910 - val_loss: 0.6663\n",
      "Epoch 55/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7584 - val_loss: 0.6664\n",
      "Epoch 56/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6708 - val_loss: 0.6664\n",
      "Epoch 57/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5944 - val_loss: 0.6668\n",
      "Epoch 58/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6356 - val_loss: 0.6674\n",
      "Epoch 59/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6973 - val_loss: 0.6676\n",
      "Epoch 60/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8276 - val_loss: 0.6677\n",
      "Epoch 61/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8048 - val_loss: 0.6675\n",
      "Epoch 62/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5635 - val_loss: 0.6675\n",
      "Epoch 63/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6816 - val_loss: 0.6679\n",
      "Epoch 64/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6037 - val_loss: 0.6681\n",
      "Epoch 65/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5507 - val_loss: 0.6684\n",
      "Epoch 66/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5685 - val_loss: 0.6687\n",
      "Epoch 67/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5836 - val_loss: 0.6692\n",
      "Epoch 67: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16bb63250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         verbose=1,callbacks=[early_stop,board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86f67156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/fit/2024-03-07==1803\n"
     ]
    }
   ],
   "source": [
    "print(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb498fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tylercoen/Desktop/Certifications/Coding Courses/Python/Python for Data Science and Machine Learning Bootcamp/Neural Nets and Deep Learning'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:6006/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
